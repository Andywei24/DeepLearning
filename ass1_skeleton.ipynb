{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 2AMM10 2023-2024\n",
    "\n",
    "## Group: [Fill in your group name]\n",
    "### Member 1: [Fill in your name]\n",
    "### Member 2: [Fill in your name]\n",
    "### Member 3: [Fill in your name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import kagglehub\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# add additonal imports here\n",
    "class FashionDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir,column_class=\"articleTypeId\", transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the CSV file with labels.\n",
    "            img_dir (str): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_file)  # load CSV file\n",
    "        self.img_dir = img_dir  # image folder path\n",
    "        self.transform = transform  # image transformations\n",
    "        self.targets = list(self.df[column_class].values)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, f\"{self.df.loc[idx,'imageId']}.jpg\")  # Get image filename\n",
    "        image = Image.open(img_name).convert(\"RGB\")  # Load image\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply transformations\n",
    "\n",
    "        return image, self.targets[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data with kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "dataset_path = kagglehub.dataset_download(\"paramaggarwal/fashion-product-images-small\")\n",
    "img_dir = os.path.join(dataset_path,\"images\")\n",
    "\n",
    "# shutil.rmtree(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different datasets can be loaded using the class `FashionDataset` which is a custon PyTorch dataset (see [Datasets & DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) for more information). Below an example of how to use the `FashionDataset` constructor as well as some visualizations. Please note that you may have to adapt the arguments to match the strucucture of your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FashionDataset(\"dataset/train.csv\",img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imageId</th>\n",
       "      <th>articleTypeName</th>\n",
       "      <th>categoryName</th>\n",
       "      <th>articleTypeId</th>\n",
       "      <th>categoryId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35180</td>\n",
       "      <td>Backpacks</td>\n",
       "      <td>Bags</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33585</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9464</td>\n",
       "      <td>Shirts</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8194</td>\n",
       "      <td>Shirts</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42231</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   imageId articleTypeName categoryName  articleTypeId  categoryId\n",
       "0    35180       Backpacks         Bags             15           2\n",
       "1    33585         Tshirts      Topwear              0           0\n",
       "2     9464          Shirts      Topwear              1           0\n",
       "3     8194          Shirts      Topwear              1           0\n",
       "4    42231            Tops      Topwear              6           0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random_indices = np.random.choice(len(dataset),3)\n",
    "\n",
    "for i in random_indices:\n",
    "    img, label = dataset[i]\n",
    "    plt.title(dataset.df.iloc[i][\"articleTypeName\"]+f\" ({label.item()})\")\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomRotation(degrees=15),\n",
    "#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                          std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "train_dataset = FashionDataset(\"dataset/train.csv\",img_dir,transform=transform)\n",
    "main_test_dataset =  FashionDataset(\"dataset/main_test.csv\",img_dir,transform=transform)\n",
    "new_test_dataset =  FashionDataset(\"dataset/new_test.csv\",img_dir,transform=transform)\n",
    "main_support_dataset =  FashionDataset(\"dataset/main_support.csv\",img_dir,transform=transform)\n",
    "new_support_dataset =  FashionDataset(\"dataset/new_support.csv\",img_dir,transform=transform)\n",
    "merged_test_dataset =  FashionDataset(\"dataset/merged_test.csv\",img_dir,transform=transform) # merged corresponds to main+new\n",
    "merged_support_dataset =  FashionDataset(\"dataset/merged_support.csv\",img_dir,transform=transform)\n",
    "\n",
    "# datasets with categories\n",
    "main_test_dataset_cat =  FashionDataset(\"dataset/main_test.csv\",img_dir,column_class=\"categoryId\",transform=transform)\n",
    "main_support_dataset_cat =  FashionDataset(\"dataset/main_support.csv\",img_dir,column_class=\"categoryId\",transform=transform)\n",
    "\n",
    "\n",
    "label_id_to_label_name = {i: train_dataset.df[train_dataset.df[\"articleTypeId\"]==i][\"articleTypeName\"].iloc[0] for i in range(39)} \n",
    "label_id_to_label_name.update({i: new_test_dataset.df[new_test_dataset.df[\"articleTypeId\"]==i][\"articleTypeName\"].iloc[0] for i in range(39,39+30)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution\n",
    "# train dataset: train_dataset\n",
    "# test datset: main_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network archietecture\n",
    "\n",
    "class FashionNet(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(FashionNet, self).__init__()\n",
    "\n",
    "        # convolutional layers\n",
    "        self.conv1 = torch.nn.Conv2d(3, 32, kernel_size = 3, padding = 1)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size = 3, padding = 1)\n",
    "        self.conv3 = torch.nn.Conv2d(64, 128, kernel_size = 3, padding = 1)\n",
    "\n",
    "        # batch normalization layers\n",
    "        self.bn1 = torch.nn.BatchNorm2d(32)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(64)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(128)\n",
    "\n",
    "        # pooling layer\n",
    "        self.pool = torch.nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = torch.nn.Dropout(0.25)\n",
    "\n",
    "        # fully conneted layer\n",
    "        self.fc1 = torch.nn.Linear(128 * 28 * 28, 512)\n",
    "        # self.fc1 = torch.nn.Linear(256 * 14 * 14, 512)\n",
    "        self.fc2 = torch.nn.Linear(512, num_classes)\n",
    "\n",
    "        # activation function\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolutional block\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Second convolutional block\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Third convolutional block\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers with dropout\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x # already without softmax => can be used as a feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# create data loader\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(main_test_dataset, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "num_classes = len(train_dataset.df['articleTypeId'].unique())\n",
    "\n",
    "# initialize the model\n",
    "model = FashionNet(num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Improved training function with AMP\n",
    "def train_model(model, train_loader, test_loader, device, epochs=30):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3)\n",
    "\n",
    "    # Initialize gradient scaler for AMP\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Track best model\n",
    "    best_test_acc = 0.0\n",
    "    \n",
    "    print(f\"Starting training for {epochs} epochs...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train]')\n",
    "        \n",
    "        for inputs, labels in train_pbar:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Use AMP autocast\n",
    "            with autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Scale gradients and perform backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Update weights with scaled gradients\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            train_pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{(train_correct/train_total):.4f}'\n",
    "            })\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_acc = train_correct / train_total\n",
    "        \n",
    "        # Test phase\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        test_pbar = tqdm(test_loader, desc=f'Epoch {epoch+1}/{epochs} [Test]')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_pbar:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "                test_pbar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'acc': f'{(test_correct/test_total):.4f}'\n",
    "                })\n",
    "        \n",
    "        test_loss = test_loss / len(test_loader)\n",
    "        test_acc = test_correct / test_total\n",
    "        \n",
    "        # Update learning rate based on test accuracy\n",
    "        scheduler.step(test_acc)\n",
    "        \n",
    "        # Print results\n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "        # Clean GPU cache\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Save best model\n",
    "        if test_acc > best_test_acc: \n",
    "            best_test_acc = test_acc\n",
    "            # Save the model if needed\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"New best test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    print(f\"Training completed! Best test accuracy: {best_test_acc:.4f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Using device: {device}\")\n",
    "model = train_model(model, train_loader, test_loader, device, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Regular accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    # Balanced accuracy\n",
    "    balanced_acc = 100 * balanced_accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return accuracy, balanced_acc\n",
    "\n",
    "# After training your model\n",
    "acc, bal_acc = evaluate_model(model, test_loader, device)\n",
    "print(f'Accuracy: {acc:.2f}%')\n",
    "print(f'Balanced Accuracy: {bal_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution\n",
    "# train dataset: train_dataset\n",
    "# test and support dataset: see scenarios table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triplet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, image_dataset):  # image_dataset = torchvision.datasets.ImageFolder or custom\n",
    "        self.image_dataset = image_dataset\n",
    "        self.labels = image_dataset.targets\n",
    "        self.class_to_indices = self._map_labels_to_indices()\n",
    "\n",
    "    def _map_labels_to_indices(self):\n",
    "        from collections import defaultdict\n",
    "        mapping = defaultdict(list)\n",
    "        for idx, label in enumerate(self.labels):\n",
    "            mapping[label].append(idx)\n",
    "        return mapping\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_img, anchor_label = self.image_dataset[idx]\n",
    "\n",
    "        # Positive\n",
    "        pos_idx = random.choice(self.class_to_indices[anchor_label])\n",
    "        pos_img, _ = self.image_dataset[pos_idx]\n",
    "\n",
    "        # Negative\n",
    "        neg_label = random.choice([l for l in self.class_to_indices if l != anchor_label])\n",
    "        neg_idx = random.choice(self.class_to_indices[neg_label])\n",
    "        neg_img, _ = self.image_dataset[neg_idx]\n",
    "\n",
    "        return anchor_img, pos_img, neg_img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "def train_embedding_model(model, triplet_loader, device, epochs=20, margin=1.0):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.TripletMarginLoss(margin=margin)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    print(f\"Starting embedding training for {epochs} epochs...\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        pbar = tqdm(triplet_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        for anchor, positive, negative in pbar:\n",
    "            anchor, positive, negative = (\n",
    "                anchor.to(device),\n",
    "                positive.to(device),\n",
    "                negative.to(device),\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
    "                anchor_feat = model(anchor)\n",
    "                positive_feat = model(positive)\n",
    "                negative_feat = model(negative)\n",
    "\n",
    "                loss = criterion(anchor_feat, positive_feat, negative_feat)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            pbar.set_postfix({\"triplet_loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        avg_loss = running_loss / len(triplet_loader)\n",
    "        print(f\"Epoch {epoch+1}: Avg Triplet Loss = {avg_loss:.4f}\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "triplet_dataset = TripletDataset(train_dataset)\n",
    "triplet_loader = DataLoader(triplet_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FashionNet(num_classes=39)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trained_model = train_embedding_model(model, triplet_loader, device, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"trained_triplet_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, dataloader, device):\n",
    "    model.eval()\n",
    "    features, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)  # 39D features\n",
    "            features.append(outputs.cpu())\n",
    "            labels.extend(targets.cpu().tolist())\n",
    "\n",
    "    return torch.cat(features), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neighbor classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def classify_by_nearest_neighbor(test_features, support_features, support_labels, metric='cosine'):\n",
    "    predictions = []\n",
    "\n",
    "    for test_feat in test_features:\n",
    "        if metric == 'cosine':\n",
    "            sims = F.cosine_similarity(test_feat.unsqueeze(0), support_features)\n",
    "            idx = torch.argmax(sims).item()\n",
    "        if metric == 'euclidean':\n",
    "            dists = torch.norm(support_features - test_feat.unsqueeze(0), dim=1)\n",
    "            idx = torch.argmin(dists).item()\n",
    "\n",
    "        predictions.append(support_labels[idx])\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "main_test_loader = DataLoader(main_test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "support_features, support_labels = extract_features(model, train_loader, device)\n",
    "test_features, test_labels = extract_features(model, main_test_loader, device)\n",
    "\n",
    "preds = classify_by_nearest_neighbor(test_features, support_features, support_labels)\n",
    "evaluate_predictions(test_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_support_loader = DataLoader(main_support_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "support_features, support_labels = extract_features(model, main_support_loader, device)\n",
    "test_features, test_labels = extract_features(model, main_test_loader, device)\n",
    "\n",
    "preds = classify_by_nearest_neighbor(test_features, support_features, support_labels)\n",
    "evaluate_predictions(test_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_support_loader = DataLoader(new_support_dataset, batch_size=32, shuffle=False)\n",
    "new_test_loader = DataLoader(new_test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "support_features, support_labels = extract_features(model, new_support_loader, device)\n",
    "test_features, test_labels = extract_features(model, new_test_loader, device)\n",
    "\n",
    "preds = classify_by_nearest_neighbor(test_features, support_features, support_labels)\n",
    "evaluate_predictions(test_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_support_loader = DataLoader(merged_support_dataset, batch_size=32, shuffle=False)\n",
    "merged_test_loader = DataLoader(merged_test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "support_features, support_labels = extract_features(model, merged_support_loader, device)\n",
    "test_features, test_labels = extract_features(model, merged_test_loader, device)\n",
    "\n",
    "preds = classify_by_nearest_neighbor(test_features, support_features, support_labels)\n",
    "evaluate_predictions(test_labels, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution\n",
    "# test dataset: merged_test_dataset\n",
    "# support/catalog dataset: support_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install what is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import what is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate & label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "def simulate_embeddings(num_samples, dim=128):\n",
    "    return np.random.randn(num_samples, dim)\n",
    "\n",
    "def simulate_labels(num_samples, num_classes=10):\n",
    "    return np.random.randint(0, num_classes, size=num_samples)\n",
    "\n",
    "support_embeddings = simulate_embeddings(500)\n",
    "support_labels = simulate_labels(500)\n",
    "\n",
    "test_embeddings = simulate_embeddings(100)\n",
    "test_labels = simulate_labels(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods to compute top-k and confidence values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_similarities(test_embedding, support_embeddings, k=3):\n",
    "    similarities = cosine_similarity([test_embedding], support_embeddings)[0]\n",
    "    top_k_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    return top_k_indices, similarities[top_k_indices]\n",
    "\n",
    "def compute_confidence(similarities):\n",
    "    return similarities[0] - similarities[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 50)\n",
    "error_rates = []\n",
    "coverage_rates = []\n",
    "\n",
    "for threshold in tqdm(thresholds):\n",
    "    total = len(test_embeddings)\n",
    "    shown = 0\n",
    "    incorrect = 0\n",
    "\n",
    "    for i, test_embedding in enumerate(test_embeddings):\n",
    "        top_k_indices, similarities = get_top_k_similarities(test_embedding, support_embeddings)\n",
    "        confidence = compute_confidence(similarities)\n",
    "\n",
    "        if confidence >= threshold:\n",
    "            shown += 1\n",
    "            recommended_labels = support_labels[top_k_indices]\n",
    "            if test_labels[i] not in recommended_labels:\n",
    "                incorrect += 1\n",
    "\n",
    "    coverage = shown / total if total else 0\n",
    "    error_rate = incorrect / shown if shown else 0\n",
    "    coverage_rates.append(coverage * 100)\n",
    "    error_rates.append(error_rate * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(error_rates, coverage_rates, marker='o', color='orange')\n",
    "plt.xlabel('Error Rate (%)')\n",
    "plt.ylabel('Coverage (%)')\n",
    "plt.title('Error Rate vs. Coverage')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution\n",
    "# datasets: first 10 classes of train_dataset and main_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (3BENV)",
   "language": "python",
   "name": "3benv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
